# VLArm
**Goal:** Replicating Google DeepMind's VLA model (Vision-Language-Action model) to create a prototype of a robotic arm that can be used in industry settings.
The final robot will:
* Listen to voice commands.
* Take actions with the power of MCP (Model Context Protocol).
* Be 'embodied' in its actions with the power of computer vision.

## Credits
* This project, and all parts purchased, are based on the HuggingFace SO-100 tutorial and LeRobot library: https://github.com/TheRobotStudio/SO-ARM100
